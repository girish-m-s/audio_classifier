# -*- coding: utf-8 -*-
"""sc_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gzroSXDmCp9p3FINEDM645U_waX1a9ay
"""

pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121

# sc_train.py
# SpeechCommands audio classifier (end to end)
# downloads data via torchaudio and trains a small CNN
# Increase the dataset to get more accuracy, I am adjusting the dataset size based on the hardware I am running with.

import os
import time
import random
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, WeightedRandomSampler

import torchaudio
from torchaudio.datasets import SPEECHCOMMANDS


@dataclass
class RunCfg:
    where_data: str = "./_sc_data"
    seed: int = 1337

    sr: int = 16000
    one_sec: int = 16000

    n_fft: int = 512
    hop: int = 160
    win: int = 400
    mel_bins: int = 64

    batch: int = 256
    workers: int = 4
    epochs: int = 18

    lr: float = 3e-4
    wd: float = 1e-3

    use_amp: bool = True
    use_sampler: bool = True
    use_weighted_loss: bool = False  # sampler usually enough

    mix_p: float = 0.25
    mix_a: float = 0.4

    shift_p: float = 0.6
    noise_p: float = 0.6
    mask_p: float = 0.5

    try_compile: bool = True


cfg = RunCfg()


def pick_device():
    return "cuda" if torch.cuda.is_available() else "cpu"


def set_seed(x):
    random.seed(x)
    torch.manual_seed(x)
    torch.cuda.manual_seed_all(x)


class SCPart(SPEECHCOMMANDS):
    # official split lists from the dataset folder
    def __init__(self, root, part=None, download=True):
        super().__init__(root, download=download)
        self.part = part

        def load_list(fname):
            p = os.path.join(self._path, fname)
            with open(p, "r") as f:
                lines = [os.path.normpath(os.path.join(self._path, ln.strip())) for ln in f]
            return set(lines)

        if part == "validation":
            keep = load_list("validation_list.txt")
            self._walker = [w for w in self._walker if w in keep]
        elif part == "testing":
            keep = load_list("testing_list.txt")
            self._walker = [w for w in self._walker if w in keep]
        elif part == "training":
            val = load_list("validation_list.txt")
            test = load_list("testing_list.txt")
            self._walker = [w for w in self._walker if w not in val and w not in test]


def fix_len(w, n):
    # pad or cut. keep it simple
    t = w.size(-1)
    if t == n:
        return w
    if t > n:
        return w[..., :n]
    return F.pad(w, (0, n - t))


def aug_shift(w, frac=0.15):
    t = w.size(-1)
    m = int(t * frac)
    if m < 1:
        return w
    s = random.randint(-m, m)
    if s == 0:
        return w
    return torch.roll(w, shifts=s, dims=-1)


def aug_noise(w, snr_lo=10.0, snr_hi=25.0):
    snr = random.uniform(snr_lo, snr_hi)
    sig = w.pow(2).mean().clamp_min(1e-8)
    npow = sig / (10 ** (snr / 10))
    n = torch.randn_like(w) * npow.sqrt()
    return w + n


def mask_spec(mel, t_mask=18, f_mask=10):
    # mel: [M, T]
    m, tt = mel.size(0), mel.size(1)

    if tt > 2:
        t0 = random.randint(0, tt - 1)
        t1 = min(tt, t0 + random.randint(0, t_mask))
        mel[:, t0:t1] = 0

    if m > 2:
        f0 = random.randint(0, m - 1)
        f1 = min(m, f0 + random.randint(0, f_mask))
        mel[f0:f1, :] = 0

    return mel


def do_mix(x, y, n_classes, a=0.4):
    if a <= 0:
        return x, F.one_hot(y, num_classes=n_classes).float()

    lam = torch.distributions.Beta(a, a).sample().item()
    idx = torch.randperm(x.size(0), device=x.device)

    x2 = x[idx]
    y2 = y[idx]

    y1h = F.one_hot(y, num_classes=n_classes).float()
    y2h = F.one_hot(y2, num_classes=n_classes).float()

    x = lam * x + (1.0 - lam) * x2
    ysoft = lam * y1h + (1.0 - lam) * y2h
    return x, ysoft


class MelStuff(nn.Module):
    def __init__(self, sr, n_fft, hop, win, mel_bins):
        super().__init__()
        self.mel = torchaudio.transforms.MelSpectrogram(
            sample_rate=sr,
            n_fft=n_fft,
            hop_length=hop,
            win_length=win,
            n_mels=mel_bins,
            center=True,
            power=2.0,
            f_min=50.0,
            f_max=sr / 2.0,
            norm="slaney",
            mel_scale="htk",
        )
        self.to_db = torchaudio.transforms.AmplitudeToDB(stype="power", top_db=80)

    def forward(self, wav):
        # wav: [B, 1, T]
        x = self.mel(wav)          # [B, M, TT]
        x = self.to_db(x)
        x = (x - x.mean(dim=(-2, -1), keepdim=True)) / (x.std(dim=(-2, -1), keepdim=True) + 1e-5)
        return x


class TinyCNN(nn.Module):
    def __init__(self, n_out):
        super().__init__()
        self.body = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),

            nn.Conv2d(128, 192, 3, padding=1),
            nn.BatchNorm2d(192),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1)),
        )
        self.head = nn.Linear(192, n_out)

    def forward(self, x):
        z = self.body(x).flatten(1)
        return self.head(z)


def get_labels(ds):
    # a bit slow but ok once
    names = sorted(list({ds[i][2] for i in range(len(ds))}))
    to_id = {s: i for i, s in enumerate(names)}
    return names, to_id


def make_collate(to_id, is_train, mel_cpu):
    def _go(batch):
        xs = []
        ys = []

        for wav, sr, lab, _, _ in batch:
            if sr != cfg.sr:
                wav = torchaudio.functional.resample(wav, sr, cfg.sr)
            wav = fix_len(wav, cfg.one_sec)

            if is_train:
                if random.random() < cfg.shift_p:
                    wav = aug_shift(wav)
                if random.random() < cfg.noise_p:
                    wav = aug_noise(wav)

            xs.append(wav)
            ys.append(to_id[lab])

        wavs = torch.stack(xs, 0)                   # [B, 1, T]
        y = torch.tensor(ys, dtype=torch.long)

        # do mel on CPU in workers
        with torch.no_grad():
            mel = mel_cpu(wavs)                     # [B, M, TT]

        mel = mel.unsqueeze(1).contiguous()         # [B, 1, M, TT]

        if is_train and random.random() < cfg.mask_p:
            for i in range(mel.size(0)):
                mel[i, 0] = mask_spec(mel[i, 0])

        return mel, y

    return _go


def make_sampler(ds, to_id):
    counts = torch.zeros(len(to_id), dtype=torch.long)
    for i in range(len(ds)):
        counts[to_id[ds[i][2]]] += 1

    inv = 1.0 / counts.float().clamp_min(1.0)

    w = torch.zeros(len(ds), dtype=torch.float)
    for i in range(len(ds)):
        w[i] = inv[to_id[ds[i][2]]]

    samp = WeightedRandomSampler(weights=w, num_samples=len(ds), replacement=True)
    return samp, counts, inv


@torch.no_grad()
def eval_pass(net, loader, dev):
    net.eval()
    seen = 0
    good = 0
    loss_sum = 0.0

    for x, y in loader:
        x = x.to(dev, non_blocking=True)
        y = y.to(dev, non_blocking=True)

        out = net(x)
        loss = F.cross_entropy(out, y)

        pred = out.argmax(1)
        good += (pred == y).sum().item()
        seen += y.numel()
        loss_sum += loss.item() * y.size(0)

    return loss_sum / max(1, seen), good / max(1, seen)


def train_pass(net, loader, opt, scaler, dev, n_classes, class_w=None):
    net.train()
    t0 = time.time()

    seen = 0
    good = 0
    loss_sum = 0.0

    for x, y in loader:
        x = x.to(dev, non_blocking=True)
        y = y.to(dev, non_blocking=True)

        ysoft = None
        if random.random() < cfg.mix_p:
            x, ysoft = do_mix(x, y, n_classes, a=cfg.mix_a)

        opt.zero_grad(set_to_none=True)

        use_amp = cfg.use_amp and dev.startswith("cuda")
        with torch.cuda.amp.autocast(enabled=use_amp):
            out = net(x)
            if ysoft is None:
                loss = F.cross_entropy(out, y, weight=class_w)
            else:
                lp = F.log_softmax(out, dim=1)
                loss = -(ysoft * lp).sum(1).mean()

        scaler.scale(loss).backward()
        scaler.step(opt)
        scaler.update()

        with torch.no_grad():
            if ysoft is None:
                pred = out.argmax(1)
                good += (pred == y).sum().item()
                seen += y.numel()
            else:
                seen += y.numel()
            loss_sum += loss.item() * y.size(0)

    dt = time.time() - t0
    return loss_sum / max(1, seen), good / max(1, seen), dt


@torch.no_grad()
def quick_infer(net, mel_gpu, wav, dev, label_names):
    # wav: [1, T] or [T]
    if wav.dim() == 1:
        wav = wav.unsqueeze(0)
    wav = fix_len(wav, cfg.one_sec).unsqueeze(0)  # [1, 1, T]
    wav = wav.to(dev)

    mel = mel_gpu(wav).unsqueeze(1)               # [1, 1, M, TT]
    out = net(mel)
    p = out.softmax(1)[0]
    k = int(p.argmax().item())
    return label_names[k], float(p[k].item())


def main():
    set_seed(cfg.seed)
    dev = pick_device()

    if dev.startswith("cuda"):
        torch.backends.cudnn.benchmark = True

    train_ds = SCPart(cfg.where_data, part="training", download=True)
    val_ds = SCPart(cfg.where_data, part="validation", download=True)
    test_ds = SCPart(cfg.where_data, part="testing", download=True)

    label_names, to_id = get_labels(train_ds)
    n_classes = len(label_names)
    print("num classes:", n_classes)

    mel_cpu = MelStuff(cfg.sr, cfg.n_fft, cfg.hop, cfg.win, cfg.mel_bins).cpu().eval()

    train_collate = make_collate(to_id, is_train=True, mel_cpu=mel_cpu)
    eval_collate = make_collate(to_id, is_train=False, mel_cpu=mel_cpu)

    class_w_dev = None
    sampler = None

    if cfg.use_sampler:
        sampler, counts, inv = make_sampler(train_ds, to_id)
        print("train counts:", int(counts.min()), "to", int(counts.max()))
        if cfg.use_weighted_loss:
            # optional. usually pick one method
            class_w_dev = (inv / inv.mean()).to(dev)

    if cfg.use_sampler:
        train_loader = DataLoader(
            train_ds,
            batch_size=cfg.batch,
            sampler=sampler,
            num_workers=cfg.workers,
            pin_memory=True,
            drop_last=True,
            collate_fn=train_collate,
        )
    else:
        train_loader = DataLoader(
            train_ds,
            batch_size=cfg.batch,
            shuffle=True,
            num_workers=cfg.workers,
            pin_memory=True,
            drop_last=True,
            collate_fn=train_collate,
        )

    val_loader = DataLoader(
        val_ds,
        batch_size=cfg.batch,
        shuffle=False,
        num_workers=cfg.workers,
        pin_memory=True,
        collate_fn=eval_collate,
    )
    test_loader = DataLoader(
        test_ds,
        batch_size=cfg.batch,
        shuffle=False,
        num_workers=cfg.workers,
        pin_memory=True,
        collate_fn=eval_collate,
    )

    net = TinyCNN(n_out=n_classes).to(dev)

    if cfg.try_compile:
        try:
            net = torch.compile(net)
            print("compile ok")
        except Exception as e:
            print("compile skipped:", repr(e))

    opt = torch.optim.AdamW(net.parameters(), lr=cfg.lr, weight_decay=cfg.wd)
    scaler = torch.cuda.amp.GradScaler(enabled=(cfg.use_amp and dev.startswith("cuda")))

    best = 0.0
    save_to = "best_sc.pt"

    print("device:", dev)
    for ep in range(1, cfg.epochs + 1):
        tr_loss, tr_acc, dt = train_pass(net, train_loader, opt, scaler, dev, n_classes, class_w=class_w_dev)
        va_loss, va_acc = eval_pass(net, val_loader, dev)

        if va_acc > best:
            best = va_acc
            torch.save({"model": net.state_dict(), "labels": label_names, "cfg": cfg.__dict__}, save_to)

        print(f"ep {ep:02d}  tr {tr_loss:.4f}/{tr_acc:.3f}  val {va_loss:.4f}/{va_acc:.3f}  {dt:.1f}s")

    ck = torch.load(save_to, map_location="cpu")
    net.load_state_dict(ck["model"])
    net.to(dev)

    te_loss, te_acc = eval_pass(net, test_loader, dev)
    print("test:", round(te_loss, 4), round(te_acc, 4))

    # one sample check
    mel_gpu = MelStuff(cfg.sr, cfg.n_fft, cfg.hop, cfg.win, cfg.mel_bins).to(dev).eval()
    net.eval()

    wav, sr, lab, *_ = test_ds[0]
    if sr != cfg.sr:
        wav = torchaudio.functional.resample(wav, sr, cfg.sr)

    pred, conf = quick_infer(net, mel_gpu, wav, dev, label_names)
    print("example:", "true =", lab, "| pred =", pred, "| p =", round(conf, 3))

    if dev.startswith("cuda"):
        torch.cuda.synchronize()
        t0 = time.time()
        for _ in range(50):
            _ = quick_infer(net, mel_gpu, wav, dev, label_names)
        torch.cuda.synchronize()
        ms = (time.time() - t0) * 1000.0 / 50.0
        print("avg infer ms:", round(ms, 2))


if __name__ == "__main__":
    main()

